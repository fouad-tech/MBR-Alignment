import os
import argparse
import json
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from utility_func import *
from datasets import Dataset
from scipy.stats import spearmanr
from transformers import AutoModelForCausalLM, AutoTokenizer
from utils import (
    load_dataset,
    load_matrix,
    load_samples_from_file,
    result_dir,
    matrix_dir,
    prompt_dir
)  # , approx_dir, diverse_dir
from parser import get_mbr_parser

from policy.mbr import compute_score_matrix, compute_mbr


@torch.no_grad()
def compute_reward(contexts, samples, policy_model, ref_model, model_name):
  """
  Compute KL divergence between policy model and reference model, assuming that samples are sampled from the policy model
  We compute P(generation | context) instead of P(context + generation), as we find the latter may yield negative KL.
  contexts: the input to the model when generating(i.e., full prompt, context)
  samples: the samples generated by the model, including the prompt for a decoder-only model (i.e., context + generation)
  """
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  tokenizer.pad_token_id = tokenizer.eos_token_id
  ds = Dataset.from_dict({'context': contexts, 'sample': samples})
  dataloader = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=False)
  kl_divergences = []
  for batch in tqdm(dataloader):
      context = tokenizer(batch['context'], return_tensors='pt', padding=True)
      context_length = torch.sum(context.attention_mask.eq(1), dim=-1).cpu().numpy()
      model_inputs = tokenizer(batch['sample'], return_tensors='pt', padding=True, truncation=True)
      policy_inputs = model_inputs.to(policy_model.device)
      ref_inputs = model_inputs.to(ref_model.device)
      select_mask = policy_inputs.attention_mask[:,1:].eq(1)
      for i, mask_row in enumerate(select_mask):
          mask_row[:context_length[i]-1] = False
      policy_output = policy_model.forward(**policy_inputs)
      ref_output = ref_model.forward(**ref_inputs)
      # Compute log probabilities using softmax
      policy_log_probs = torch.log_softmax(policy_output.logits, dim=-1)
      policy_log_probs = torch.gather(policy_log_probs[:,:-1], -1, policy_inputs.input_ids[:,1:].unsqueeze(-1)).squeeze(-1)
      policy_log_probs = torch.masked_select(policy_log_probs, select_mask)
      ref_log_probs = torch.log_softmax(ref_output.logits, dim=-1)
      ref_log_probs = torch.gather(ref_log_probs[:,:-1], -1, ref_inputs.input_ids[:,1:].unsqueeze(-1)).squeeze(-1)
      ref_log_probs = torch.masked_select(ref_log_probs, select_mask.to(ref_log_probs.device))
      # Compute KL divergence for each sequence in the batch
      kl_divergence_batch = torch.sum((policy_log_probs - ref_log_probs.to(policy_log_probs.device)), dim=-1)
      # Add KL divergences for this batch to the list
      kl_divergences.append(kl_divergence_batch.cpu().item())
      # del inputs, policy_inputs, ref_inputs, policy_output, ref_output, policy_log_probs, ref_log_probs, kl_divergence_batch
  # Compute the average KL divergence over all samples
  #average_kl_divergence = sum(kl_divergences) / len(kl_divergences)
  #print("average KL divergence =", average_kl_divergence)
  return kl_divergences



def compute_score(df, d_best, trg, compute_evaluate, src=None):
    d_hyp = df.iloc[d_best]["text"]
    d_score = compute_evaluate(d_hyp, trg, src)
    return d_score

def computeBert(trg,samples,compute_evaluate):
    scores = []
    for i in samples:
        scores.append(compute_evaluate(i,trg,None))
    scores = np.array(scores)
    return np.argsort(scores),np.argmax(scores),np.argmin(scores)

def compute_mbr(
    hyp=None,
    compute_similatiy=None,
    matrix=None,
    weights=None,
    src=None,
    incremental=False,
):
    assert (compute_similatiy is not None) or (matrix is not None)
    if matrix is None:
        matrix = compute_score_matrix(hyp, compute_similatiy, [src] * len(hyp))

    if weights is not None:
        mbr_scores = matrix @ np.transpose(weights)
    else:
        mbr_scores = np.sum(matrix, axis=1)

    if incremental:
        best_hyp = -1
        best_score = -np.inf
        bests = []
        for i in range(mbr_scores.shape[0]):
            if mbr_scores[i] > best_score:
                best_hyp = i
                best_score = mbr_scores[i]
            assert best_hyp >= 0
            bests.append(best_hyp)
        return bests  # List of hypothesis indices.
    else:
        hyp_list = np.argsort(mbr_scores)
        best_hyp = np.argmax(mbr_scores)
        worst_hyp = np.argmin(mbr_scores)

        assert len(hyp_list) >= 0
        return hyp_list,best_hyp,worst_hyp,mbr_scores

if __name__ == "__main__":
    """
    This script is the "main function" of the experiment.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', help="dataset name", default='cnndm')
    parser.add_argument('--model_ref',default='mistralai/Mistral-7B-Instruct-v0.1')
    parser.add_argument('--policy_model',default='../model-based-mbr/DPO/cnndm/bmw/sig/BETA0.1/checkpoint-3600')
    parser.add_argument('--prompt', default='promptCNN.txt')
    parser.add_argument('--model_type',default='DPO')
    parser.add_argument('--LowerRange',default=0,type = int)
    parser.add_argument('--UpperRange',default=1000,type=int)
    parser.add_argument('--eps',default=0.02,type=float)
    parser.add_argument('--topk',default=0, type = float)
    parser.add_argument('--topp',default=1.0,type = float)
    
    args = parser.parse_args()

    dataset = args.dataset
    prompt_path=args.prompt
    UpperRange = args.UpperRange
    LowerRange = args.LowerRange
    model_ref= args.model_ref

    with open(os.path.join(prompt_dir, prompt_path), "r") as f:
            prompt = f.read()
    print(prompt)

    policy_model = AutoModelForCausalLM.from_pretrained(
                args.policy_model, torch_dtype=torch.bfloat16, device_map="auto"
            )
    tokenizer = AutoTokenizer.from_pretrained(model_ref)
    tokenizer.pad_token_id = tokenizer.eos_token_id
    ref_model = AutoModelForCausalLM.from_pretrained(model_ref,
                                                torch_dtype=torch.bfloat16,
                                                 device_map="auto"
                                               )
    epsilon = args.eps
    topk = args.topk
    topp = args.topp

    sim = 'bertscore'
    eval_func = 'bertscore'


    n_samples = 32

    # Algorithm config
    
    compute_similarity, similarity = load_similarity(sim)
    compute_distance = load_distance(sim, compute_similarity)
    compute_evaluate, evaluator = load_evaluate(eval_func, sim, similarity)
    compute_evaluate_meteor, evaluator1 = load_evaluate('meteor', sim, similarity)
    compute_evaluate_rouge, evaluator2 = load_evaluate('rouge', sim, similarity)
    compute_evaluate_rouge1, evaluator3 = load_evaluate('rouge1', sim, similarity)

    src_lines = load_dataset(dataset)  # src is used only by comet and clip.
    trg_lines = load_dataset(dataset, ref=True)

    # client = boto3.client("s3")

    model_n = os.path.basename(model_ref)

    os.makedirs(os.path.join(matrix_dir, dataset, model_n), exist_ok=True)
    sample_dir = os.path.join('./samples',dataset,model_n)

    files = sorted(os.listdir(sample_dir))

    filtered_files = load_samples_from_file(files, epsilon, topk, topp)

    assert len(filtered_files) > 0
    meteor = []
    bertscore =[]
    rouge1 = []
    rouge = []
    rows = []
    
    correlations = []
    print(len(filtered_files))
    for fileId in tqdm(range(LowerRange,UpperRange)):
        contexts =[]
        samples=[]
        filename = filtered_files[fileId]
        sample_id = int(filename.split("_")[0])
        assert "{:04}".format(sample_id) in filename

        src_input = src_lines[sample_id]
        trg = trg_lines[sample_id]

        df = pd.read_csv(os.path.join(sample_dir, filename))

        assert len(df) >= n_samples
        df = df[:n_samples]

        df.fillna(
            "", inplace=True
        )  # TODO: This is needed to remove empty strings. In reality empty strings can be ignored. probably it's better to drop.
        hyp = df.iloc[:]["text"]
        
        
        for src_output in hyp:
            if dataset == "squad_v2":

                messages = [
                    {
                        "role": "system",
                        "content": prompt,
                    },
                    {
                        "role": "user",
                        "content": src_input,
                    },
                ]
                context = tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )

                sample = context + '\n' +src_output #hypsDf[sample_id]

                contexts.append(context)
                samples.append(sample)
            else:
                        

                input_source = prompt.replace("[[QUESTION]]", src_input)
                input_source = "[INST] " + input_source + "[/INST]"
                contexts.append(input_source)
                samples.append(input_source+src_output)

        Rewards = np.array(compute_reward(contexts, samples, policy_model, ref_model,model_ref))
        hyp_list = np.argsort(Rewards)
        #r_list = r_list[::-1]
        best_r_hyp = np.argmax(Rewards)
        worst_r_hyp = np.argmin(Rewards)

        #hyp_list = hyp_list[::-1]
        r_list,bert_max, _ = computeBert(trg,hyp,compute_evaluate)


        sorted_indices_method1 = r_list
        sorted_indices_method2 = hyp_list
        ranks_method1 = np.empty_like(sorted_indices_method1)
        ranks_method2 = np.empty_like(sorted_indices_method2)
        ranks_method1[sorted_indices_method1] = np.arange(len(r_list))
        ranks_method2[sorted_indices_method2] = np.arange(len(Rewards))
        correlation, _ = spearmanr(ranks_method1, ranks_method2)
        correlations.append(correlation)




    print(f"Spearman's rank correlation coefficient for {dataset}: {sum(correlations)/len(correlations)}") 
    with open('../model-based-mbr/spearmansOracle.txt', 'a') as file:
        # Append the data to the file
        file.write(f"Spearman's rank correlation coefficient {args.model_type} for {dataset}: {sum(correlations)/len(correlations)}\n")

        





   
