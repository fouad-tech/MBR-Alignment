import torch
from tqdm import tqdm
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
import os 
import pandas as pd
import argparse
from utils import (
    load_dataset,
    load_matrix,
    load_kwargs,
    load_samples_from_file,
    result_dir,
    matrix_dir,
    prompt_dir,
)  # , approx_dir, diverse

def compute_evaluateN(hyp, ref):

      evaluator = evaluate.load('meteor')
      scores = [
                (evaluator.compute(predictions=[hyp], references=[r])["meteor"],r)
                for r in ref
            ]
      return max(scores)


@torch.no_grad()
def compute_kl_div_from_samples(contexts, samples, policy_model, ref_model, model_name):
  """
  Compute KL divergence between policy model and reference model, assuming that samples are sampled from the policy model
  We compute P(generation | context) instead of P(context + generation), as we find the latter may yield negative KL.
  contexts: the input to the model when generating(i.e., full prompt, context)
  samples: the samples generated by the model, including the prompt for a decoder-only model (i.e., context + generation)
  """
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  tokenizer.pad_token_id = tokenizer.eos_token_id
  ds = Dataset.from_dict({'context': contexts, 'sample': samples})
  dataloader = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=False)
  kl_divergences = []
  c =0
  for batch in tqdm(dataloader):
      context = tokenizer(batch['context'], return_tensors='pt', padding=True)
      context_length = torch.sum(context.attention_mask.eq(1), dim=-1).cpu().numpy()
      model_inputs = tokenizer(batch['sample'], return_tensors='pt', padding=True, truncation=True)
      policy_inputs = model_inputs.to(policy_model.device)
      ref_inputs = model_inputs.to(ref_model.device)
      select_mask = policy_inputs.attention_mask[:,1:].eq(1)

      for i, mask_row in enumerate(select_mask):
          mask_row[:context_length[i]-1] = False
      policy_output = policy_model.forward(**policy_inputs)
      ref_output = ref_model.forward(**ref_inputs)
      #breakpoint()
      # Compute log probabilities using softmax
      policy_log_probs = torch.log_softmax(policy_output.logits, dim=-1)
      policy_log_probs = torch.gather(policy_log_probs[:,:-1], -1, policy_inputs.input_ids[:,1:].unsqueeze(-1)).squeeze(-1)
      policy_log_probs = torch.masked_select(policy_log_probs, select_mask)
      ref_log_probs = torch.log_softmax(ref_output.logits, dim=-1)
      ref_log_probs = torch.gather(ref_log_probs[:,:-1], -1, ref_inputs.input_ids[:,1:].unsqueeze(-1)).squeeze(-1)
      ref_log_probs = torch.masked_select(ref_log_probs, select_mask.to(ref_log_probs.device))
      # Compute KL divergence for each sequence in the batch
      kl_divergence_batch = torch.sum((policy_log_probs - ref_log_probs.to(policy_log_probs.device)), dim=-1)
      # Add KL divergences for this batch to the list
      #print('policy_log_probs',policy_log_probs)
      #print('ref_log_probs',ref_log_probs)
      #print('policy_log_probs',torch.sum(policy_log_probs, dim=-1))
      #print('ref_log_probs',torch.sum(ref_log_probs.to(policy_log_probs.device), dim=-1))
      if kl_divergence_batch.cpu().item()<0:
        c+=1
      kl_divergences.append(kl_divergence_batch.cpu().item())
      # del inputs, policy_inputs, ref_inputs, policy_output, ref_output, policy_log_probs, ref_log_probs, kl_divergence_batch
  # Compute the average KL divergence over all samples
  average_kl_divergence = sum(kl_divergences) / len(kl_divergences)
  print("average KL divergence =", average_kl_divergence)
  print('the number of negative r is: ',c )
  return average_kl_divergence, kl_divergences

def get_texts(tokenizer, outputs, input_length):
    """
    This function is only compatible with langauge models. not for seq2seq
    """
    bsz = outputs.sequences.shape[0]
    output_texts = []
    for b in range(bsz):
        output_text = tokenizer.decode(
            outputs.sequences[b][input_length:], skip_special_tokens=True
        )
        output_texts.append(output_text)
    return output_texts
if __name__ == "__main__":
    """
    This script is the "main function" of the experiment.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', help="dataset name", default='cnndm')
    parser.add_argument('--tag', help="dataset tag", default='bw')
    parser.add_argument('--model_type', default='DPO')
    parser.add_argument('--model_ref',default='mistralai/Mistral-7B-Instruct-v0.1')
    parser.add_argument('--model_path',default='../model-based-mbr/DPO/cnndm/bw/sig/BETA0.1/checkpoint-1800')
    parser.add_argument('--loss_type', default='sig')
    parser.add_argument('--beta',type=float,default=0.1)
    parser.add_argument('--prompt', default='promptCNN.txt')
    parser.add_argument('--LowerRange',default=0,type = int)
    parser.add_argument('--UpperRange',default=1000,type=int)
    
    args = parser.parse_args()

    dataset = args.dataset
    tag = args.tag
    model_path = args.model_path
    model_type=args.model_type
    loss_type= args.loss_type
    beta=args.beta
    prompt_path=args.prompt
    UpperRange = args.UpperRange
    LowerRange = args.LowerRange
    model_ref= args.model_ref


    with open(os.path.join(prompt_dir, prompt_path), "r") as f:
            prompt = f.read()
    print(prompt)
    # Algorithm config

    if dataset=='strategyqaT':
        src_lines = load_dataset('strategyqaV').tolist()
        trg_lines = load_dataset('strategyqaV', ref=True).tolist()
    elif dataset=='squad_v2T':
        src_lines = load_dataset('squad_v2')
        trg_lines = load_dataset('squad_v2', ref=True)
    else:
        src_lines = load_dataset(dataset) 
        trg_lines = load_dataset(dataset, ref=True)

    hypPath = os.path.join('./', model_type+"_results",dataset,tag,"BETA{}".format(beta),'samples_1000.csv'.format(UpperRange))
    
    print("Hypothesis path")
    print(hypPath)
    hypsDf = pd.read_csv(hypPath)
    hypsDf.fillna(
            "", inplace=True
        )  # TODO: This is needed to remove empty strings. In reality empty strings can be ignored. probably it's better to drop.
    hypsDf = hypsDf.iloc[:]["hypothesis"]
    
    print("the length of the datafram of hypothesis")
    print(len(hypsDf))

    #defining the model path
    #'./DPO/cnndm/bw/sig/BETA0.1'
    print("policy_model")
    print(model_path)
    #contexts, samples, policy_model, ref_model, model_name
    policy_model = AutoModelForCausalLM.from_pretrained(
        model_path,
                torch_dtype=torch.bfloat16,
                 device_map="auto"
            )
    contexts = []
    samples = []
    model_kwargs = load_kwargs(dataset)
    tokenizer = AutoTokenizer.from_pretrained(model_ref)
    tokenizer.pad_token_id = tokenizer.eos_token_id
    ref_model = AutoModelForCausalLM.from_pretrained(model_ref,
                                                torch_dtype=torch.bfloat16,
                                                 device_map="auto"
                                               )
    for sample_id in tqdm(range(LowerRange,UpperRange)):

            #src_input = src_lines[sample_id]
            #src_output = hypsDf[sample_id:sample_id+10]
            src_input = src_lines[sample_id]
            src_output = hypsDf[sample_id]

            if dataset == "squad_v2T":

                messages = [
                    {
                        "role": "system",
                        "content": prompt,
                    },
                    {
                        "role": "user",
                        "content": src_input,
                    },
                ]
                context = tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )

                sample = context + src_output #hypsDf[sample_id]

                contexts.append(context)
                samples.append(sample)
            else:
                        

                input_source = prompt.replace("[[QUESTION]]", src_input)
                input_source = "[INST] " + input_source + "[/INST]"
                contexts.append(input_source)
                samples.append(input_source+src_output)
                '''
                input_source = [prompt.replace("[[QUESTION]]", dataset_line) for dataset_line in src_input]
                input_source = ["[INST] " + dataset_line + "[/INST]" for dataset_line in input_source]
                
                model_kwargs = dict(max_new_tokens=155, no_repeat_ngram_size=4)
                model_inputs = tokenizer(
                        input_source, return_tensors="pt", return_token_type_ids=False,padding = True
                    ).to(policy_model.device)
                print('mode device',policy_model.device)
                input_length = model_inputs["input_ids"].shape[1]
                sample_output = policy_model.generate(
                           
                                **model_inputs,
                                **model_kwargs,
                                do_sample=False,
                                num_beams=1,
                                num_return_sequences=1,
                                num_beam_groups=1,
                                stopping_criteria=None,
                                return_dict_in_generate=True,
                                output_scores=True,
                                forced_bos_token_id=policy_model.config.forced_bos_token_id,
                                
                            
                            )

                output_text = get_texts(tokenizer, sample_output, input_length)
                for idx,l in enumerate(input_source):
                    contexts.append(l)
                    samples.append(l + output_text[idx])

                #context = input_source
                #sample = context + output_text[0]
            
                #contexts.append(context)
                #samples.append(sample)'''



    #print(contexts[0])
    #print(samples[0])
    
    compute_kl_div_from_samples(contexts, samples, policy_model, ref_model, model_ref)            
    
