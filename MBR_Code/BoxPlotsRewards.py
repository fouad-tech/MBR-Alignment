import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import argparse
import os
import json
from datasets import Dataset
import math
import torch
import evaluate
from transformers import AutoModelForCausalLM, AutoConfig ,AutoTokenizer, HfArgumentParser, TrainingArguments
from utility_func import *
from utils import (
    load_dataset,
    load_matrix,
    load_samples_from_file,
    result_dir,
    matrix_dir,
    prompt_dir,
)  # , approx_dir, diverse_dir
from tqdm import tqdm


def compute_mbr(
    hyp=None,
    compute_similatiy=None,
    matrix=None,
    weights=None,
    src=None,
    incremental=False,
):
    assert (compute_similatiy is not None) or (matrix is not None)
    if matrix is None:
        matrix = compute_score_matrix(hyp, compute_similatiy, [src] * len(hyp))

    if weights is not None:
        mbr_scores = matrix @ np.transpose(weights)
    else:
        mbr_scores = np.sum(matrix, axis=1)

    if incremental:
        best_hyp = -1
        best_score = -np.inf
        bests = []
        for i in range(mbr_scores.shape[0]):
            if mbr_scores[i] > best_score:
                best_hyp = i
                best_score = mbr_scores[i]
            assert best_hyp >= 0
            bests.append(best_hyp)
        return bests  # List of hypothesis indices.
    else:
        hyp_list = np.argsort(mbr_scores)
        best_hyp = np.argmax(mbr_scores)
        worst_hyp = np.argmin(mbr_scores)

        assert len(hyp_list) >= 0
        return hyp_list,best_hyp,worst_hyp


def plot(Rewards,path):
    
    margins = [np.array(r) for r in Rewards]

    # Step 3: Create the box plots
    plt.figure(figsize=(10, 6))  # Create a figure with a specific size

    # Creating box plots for the two data arrays
    plt.boxplot(margins, patch_artist=True, boxprops=dict(facecolor='skyblue'))

    # Step 4: Customize the plot
    plt.xlabel("epoch")
    plt.ylabel('Reward Margins')
    plt.xticks([0,1,2,3,4],[0, 0.25,0.5,0.75,1])  # Label the x-axis with the names of the data sets

    # Step 5: Save the plot
    plt.savefig(path)  # Save the figure as a .png file
    plt.show()  # Display the plot

@torch.no_grad()
def compute_kl_div_from_samples(contexts, samples, policy_model, ref_model, model_name):
  """
  Compute KL divergence between policy model and reference model, assuming that samples are sampled from the policy model
  We compute P(generation | context) instead of P(context + generation), as we find the latter may yield negative KL.
  contexts: the input to the model when generating(i.e., full prompt, context)
  samples: the samples generated by the model, including the prompt for a decoder-only model (i.e., context + generation)
  """
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  tokenizer.pad_token_id = tokenizer.eos_token_id
  ds = Dataset.from_dict({'context': contexts, 'sample': samples})
  dataloader = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=False)
  kl_divergences = []
  c =0
  for batch in tqdm(dataloader):
      context = tokenizer(batch['context'], return_tensors='pt', padding=True)
      context_length = torch.sum(context.attention_mask.eq(1), dim=-1).cpu().numpy()
      model_inputs = tokenizer(batch['sample'], return_tensors='pt', padding=True, truncation=True)
      policy_inputs = model_inputs.to(policy_model.device)
      ref_inputs = model_inputs.to(ref_model.device)
      select_mask = policy_inputs.attention_mask[:,1:].eq(1)

      for i, mask_row in enumerate(select_mask):
          mask_row[:context_length[i]-1] = False
      policy_output = policy_model.forward(**policy_inputs)
      ref_output = ref_model.forward(**ref_inputs)
      #breakpoint()
      # Compute log probabilities using softmax
      policy_log_probs = torch.log_softmax(policy_output.logits, dim=-1)
      policy_log_probs = torch.gather(policy_log_probs[:,:-1], -1, policy_inputs.input_ids[:,1:].unsqueeze(-1)).squeeze(-1)
      policy_log_probs = torch.masked_select(policy_log_probs, select_mask)
      ref_log_probs = torch.log_softmax(ref_output.logits, dim=-1)
      ref_log_probs = torch.gather(ref_log_probs[:,:-1], -1, ref_inputs.input_ids[:,1:].unsqueeze(-1)).squeeze(-1)
      ref_log_probs = torch.masked_select(ref_log_probs, select_mask.to(ref_log_probs.device))
      # Compute KL divergence for each sequence in the batch
    
      kl_divergence_batch = torch.sum((policy_log_probs - ref_log_probs.to(policy_log_probs.device)), dim=-1)
      
      # Add KL divergences for this batch to the list
      #print('policy_log_probs',policy_log_probs)
      #print('ref_log_probs',ref_log_probs)
      #print('policy_log_probs',torch.sum(policy_log_probs, dim=-1))
      #print('ref_log_probs',torch.sum(ref_log_probs.to(policy_log_probs.device), dim=-1))
      if kl_divergence_batch.cpu().item()<0:
        c+=1
      kl_divergences.append(kl_divergence_batch.cpu().item())
      # del inputs, policy_inputs, ref_inputs, policy_output, ref_output, policy_log_probs, ref_log_probs, kl_divergence_batch
  # Compute the average KL divergence over all samples
  average_kl_divergence = sum(kl_divergences) / len(kl_divergences)
  print("average KL divergence =", average_kl_divergence)
  print('the number of negative r is: ',c )
  return average_kl_divergence, kl_divergences


if __name__ == "__main__":
    """
    This script is the "main function" of the experiment.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument('--BaseDir', default='../model-based-mbr/')
    parser.add_argument('--model_type', default='DPO')
    parser.add_argument('--tag',default='bw')
    parser.add_argument('--dataset',default='cnndm')
    parser.add_argument('--prompt',default='promptCNN.txt')
    parser.add_argument('--eps',default=0.02, type = float)
    parser.add_argument('--topk',default=0,type = int)
    parser.add_argument('--topp',default=1.0,type = float)
    parser.add_argument('--beta',default=0.1,type = float)
    parser.add_argument('--model_name',default='mistralai/Mistral-7B-Instruct-v0.1')
    parser.add_argument('--n_samples',default=32,type=int)
    parser.add_argument('--sim',default='bertscore')
    parser.add_argument('--UpperRange',default =1000, type =int)
    parser.add_argument('--LowerRange', default =0,type=int)
    parser.add_argument('--recompute', default =0,type=int)

    args = parser.parse_args()
    BaseDir = args.BaseDir
    model_type = args.model_type
    tag = args.tag
    dataset = args.dataset
    model_name = args.model_name
    model_n = os.path.basename(model_name)
    prompt_path= args.prompt
    with open(os.path.join(prompt_dir, prompt_path), "r") as f:
            prompt = f.read()
    n_samples = args.n_samples
    beta = args.beta
    recompute = args.recompute
    epsilon = args.eps
    topk = args.topk
    topp = args.topp
    sim = args.sim
    UpperRange = args.UpperRange
    LowerRange = args.LowerRange
    sample_dir =os.path.join('./samples',dataset,model_n)
    src_lines = load_dataset(dataset)  # src is used only by comet and clip.
    trg_lines = load_dataset(dataset, ref=True) 
    loss_type = 'sig'
    files = sorted(os.listdir(sample_dir))

    filtered_files = load_samples_from_file(files, epsilon, topk, topp)
    
    tokenizer = AutoTokenizer.from_pretrained(
                model_name, padding_size="left", use_fast=True
            )
    ref_model = AutoModelForCausalLM.from_pretrained(
                model_name, load_in_4bit=True, device_map="auto"
            )
    assert len(filtered_files) > 0

    print("first 10 files=", filtered_files[:10])
    BestSamples = []
    WorstSamples = []
    MiddleSamples = []
    contexts = []
    if recompute:
        for id in tqdm(range(LowerRange,UpperRange)):

            filename = filtered_files[id]
            sample_id = int(filename.split("_")[0])
            assert "{:04}".format(sample_id) in filename


            src_input = src_lines[sample_id]
            trg = trg_lines[sample_id]

            df = pd.read_csv(os.path.join(sample_dir, filename))

            assert len(df) >= n_samples
            df = df[:n_samples]

            df.fillna(
                "", inplace=True
            )  # TODO: This is needed to remove empty strings. In reality empty strings can be ignored. probably it's better to drop.
            hyp = df.iloc[:]["text"]

            
            matrix = load_matrix(
                    os.path.join(matrix_dir, dataset, model_n), filename, sim, n_samples
                )

            ranking,best_hyp,worst_hyp = compute_mbr(matrix=matrix)
            hypsRanked = [df.iloc[i]["text"] for i in ranking]
            hypsRanked = hypsRanked[::-1]
            middleText = hypsRanked[math.ceil(len(hypsRanked)/2)]
            bestText = hypsRanked[0]
            worstText = hypsRanked[-1]

            if dataset == 'squad_v2':
                messages = [
                        {
                            "role": "system",
                            "content": prompt,
                        },
                        {
                            "role": "user",
                            "content": src_input,
                        },
                    ]
                context = tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )

                TopMessage = [
                        {
                            "role": "system",
                            "content": prompt,
                        },
                        {
                            "role": "user",
                            "content": src_input,
                        },
                        {
                            "role":"assistant",
                            'content':bestText
                            }
                    ]
                TopMessage = tokenizer.apply_chat_template(
                        TopMessage, tokenize=False, add_generation_prompt=True
                    )

                MiddleMessage = [
                        {
                            "role": "system",
                            "content": prompt,
                        },
                        {
                            "role": "user",
                            "content": src_input,
                        },
                        {
                            "role":"assistant",
                            'content':middleText
                            }
                    ]
                MiddleMessage = tokenizer.apply_chat_template(
                        MiddleMessage, tokenize=False, add_generation_prompt=True
                    )

                WorstMessage = [
                        {
                            "role": "system",
                            "content": prompt,
                        },
                        {
                            "role": "user",
                            "content": src_input,
                        },
                        {
                            "role":"assistant",
                            'content':worstText
                            }
                    ]
                WorstMessage = tokenizer.apply_chat_template(
                        WorstMessage, tokenize=False, add_generation_prompt=True
                    )
                contexts.append(context)
                WorstSamples.append(WorstMessage)
                BestSamples.append(TopMessage)
                MiddleSamples.append(MiddleMessage)

            else:

                context = prompt.replace("[[QUESTION]]", src_input)
                context = "[INST] " + context + "[/INST]"
                TopMessage = context+bestText
                WorstMessage = context + worstText
                MiddleMessage = context + middleText
                WorstSamples.append(WorstMessage)
                BestSamples.append(TopMessage)
                MiddleSamples.append(MiddleMessage)
                contexts.append(context)
        
        if dataset=='strategyqaV':
            dataset = 'strategyqaT'
        elif dataset == 'squad_v2':
            dataset = 'squad_v2T'
        model_path = os.path.join('./', model_type,dataset,tag,loss_type,"BETA{}".format(beta))
        models = [model_path+'/'+i for i in os.listdir(model_path) if 'checkpoint' in i]
        models = [int(i.split('-')[1]) for i in models]
        models = sorted(models)
        models = [os.path.join(model_path,'checkpoint-{}'.format(f)) for f in models]
        print(models)
        margins = []
        bm_margins = []
        mw_margins = []
        for i,model in enumerate(models):
            print(model)
            policy_model = AutoModelForCausalLM.from_pretrained(
                    model, load_in_4bit=True, device_map="auto"
                )
            _,rewardsBest = compute_kl_div_from_samples(contexts, BestSamples, policy_model, ref_model, model_name)
            _,rewardsMiddle = compute_kl_div_from_samples(contexts, MiddleSamples, policy_model, ref_model, model_name)
            _,rewardsWorst = compute_kl_div_from_samples(contexts, WorstSamples, policy_model, ref_model, model_name)
            
            rewardMarginsBestMiddle  = [x - y for x, y in zip(rewardsBest, rewardsMiddle)]
            rewardMarginsMiddleWorst  = [x - y for x, y in zip(rewardsMiddle,rewardsWorst)]
            rmw = np.array(rewardMarginsMiddleWorst)
            rbm = np.array(rewardMarginsBestMiddle)
            np.save('../model-based-mbr/arraysBox/RBM{}_{}.npy'.format(i,dataset), rbm)
            np.save('../model-based-mbr/arraysBox/RMW{}_{}.npy'.format(i,dataset), rmw)
            rewardMargins = rewardMarginsBestMiddle+ rewardMarginsMiddleWorst
            rewardMargins = [float(beta)*y for  y in rewardMargins]
            if len(margins)==0:
                margins.append([0 for _ in rewardMargins])
                mw_margins.append([0 for _ in rewardMarginsMiddleWorst])
                bm_margins.append([0 for _ in rewardMarginsBestMiddle])
            margins.append(rewardMargins)
            bm_margins.append(rewardMarginsBestMiddle)
            mw_margins.append(rewardMarginsMiddleWorst)
    else:
        margins = []
        bm_margins = []
        mw_margins = []
        for i in range (4):
            rbm = np.load('../model-based-mbr/arraysBox/RBM{}_{}.npy'.format(i,dataset)).tolist()
            rmw = np.load('../model-based-mbr/arraysBox/RMW{}_{}.npy'.format(i,dataset)).tolist()
            if len(margins)==0:
                margins.append([0 for _ in rmw])
                mw_margins.append([0 for _ in rmw])
                bm_margins.append([0 for _ in rbm])

            rewardMarginsBestMiddle = [float(beta)*y for  y in rbm]
            rewardMarginsMiddleWorst = [float(beta)*y for  y in rmw]
            rewardMargins = rewardMarginsBestMiddle+ rewardMarginsMiddleWorst
            
            margins.append(sum(rewardMargins)/len(rewardMargins))
            bm_margins.append(sum(rewardMarginsBestMiddle)/len(rewardMarginsBestMiddle))
            mw_margins.append(sum(rewardMarginsMiddleWorst)/len(rewardMarginsMiddleWorst))

        print('mw_margins ',mw_margins)
        print('bm_margins ',bm_margins)
        print('margins ',margins)

        

  
